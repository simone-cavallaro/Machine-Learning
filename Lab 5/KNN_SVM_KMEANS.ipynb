{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e0f41c6",
   "metadata": {},
   "source": [
    "# \ud83e\uddea Classification & Clustering Lab\n",
    "## KNN, SVM, K-Means and Evaluation Metrics\n",
    "\n",
    "---\n",
    "\n",
    "**Course**: Machine Learning & AI \u2014 LUISS Guido Carli\n",
    "\n",
    "**Objective**: Learn to implement and compare supervised classifiers (KNN, SVM) and unsupervised clustering (K-Means), and evaluate them using appropriate metrics.\n",
    "\n",
    "**Dataset**: [Iris Dataset](https://archive.ics.uci.edu/ml/datasets/iris) \u2014 150 samples of iris flowers, 4 numerical features, 3 species.\n",
    "\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| `sepal length (cm)` | Length of the sepal |\n",
    "| `sepal width (cm)` | Width of the sepal |\n",
    "| `petal length (cm)` | Length of the petal |\n",
    "| `petal width (cm)` | Width of the petal |\n",
    "| `species` | Target: setosa (0), versicolor (1), virginica (2) |\n",
    "\n",
    "**What you will learn**:\n",
    "- How K-Nearest Neighbors works and how to tune K\n",
    "- How Support Vector Machines find optimal decision boundaries\n",
    "- How K-Means discovers clusters without labels\n",
    "- How to evaluate models with precision, recall, F1-score, confusion matrix, silhouette score\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/41/Iris_versicolor_3.jpg/800px-Iris_versicolor_3.jpg\" width=\"350\" alt=\"Iris flower\">\n",
    "\n",
    "*Image: Iris versicolor \u2014 one of the three species in the dataset (Wikimedia Commons)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60843a3f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83d\udccb Instructions\n",
    "\n",
    "- Complete each **Task** by writing code in the empty cells below\n",
    "- Read the theory sections carefully before attempting each task\n",
    "- Run cells in order \u2014 later tasks depend on earlier results\n",
    "- If you are stuck, refer to the scikit-learn documentation: [https://scikit-learn.org](https://scikit-learn.org/stable/)\n",
    "- At the end, you will compare all models and discuss the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5a0c51",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83d\udce6 Part 0: Setup and Data Loading\n",
    "\n",
    "### Theory: The Iris Dataset\n",
    "\n",
    "The **Iris dataset** is one of the most famous datasets in machine learning, introduced by statistician **Ronald Fisher** in 1936.\n",
    "\n",
    "It contains measurements of **150 iris flowers** from three species:\n",
    "\n",
    "| Species | Samples | Separability |\n",
    "|---------|---------|--------------|\n",
    "| Setosa | 50 | Easily separable from the others |\n",
    "| Versicolor | 50 | Overlaps with Virginica |\n",
    "| Virginica | 50 | Overlaps with Versicolor |\n",
    "\n",
    "Each sample has **4 numerical features**: sepal length, sepal width, petal length, and petal width (all in centimeters).\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/5/56/Kosaciec_szczecinkowaty_Iris_setosa.jpg/440px-Kosaciec_szczecinkowaty_Iris_setosa.jpg\" width=\"220\" style=\"display:inline\"> <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/41/Iris_versicolor_3.jpg/440px-Iris_versicolor_3.jpg\" width=\"220\" style=\"display:inline\"> <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/9f/Iris_virginica.jpg/440px-Iris_virginica.jpg\" width=\"220\" style=\"display:inline\">\n",
    "\n",
    "*Left to right: Iris setosa, Iris versicolor, Iris virginica (Wikimedia Commons)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fde8e5",
   "metadata": {},
   "source": [
    "### \u270f\ufe0f Task 0.1: Import Libraries and Load Data\n",
    "\n",
    "Import the following libraries and load the Iris dataset:\n",
    "- `numpy`, `pandas`, `matplotlib.pyplot`, `seaborn`\n",
    "- `load_iris` from `sklearn.datasets`\n",
    "\n",
    "Create a DataFrame with the iris features and add columns for the numeric species label and the species name.\n",
    "\n",
    "Print the **shape**, the **feature names**, the **class names**, and the **count of samples per class**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949260cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2873bd6",
   "metadata": {},
   "source": [
    "### \u270f\ufe0f Task 0.2: Visualize the Data\n",
    "\n",
    "Create a figure with **two scatter plots side by side**:\n",
    "\n",
    "1. **Left plot**: Petal length vs petal width, colored by species\n",
    "2. **Right plot**: Sepal length vs sepal width, colored by species\n",
    "\n",
    "Add axis labels, a title, and a legend to each plot.\n",
    "\n",
    "> **Question**: Which pair of features separates the classes better? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acca2596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040a3b94",
   "metadata": {},
   "source": [
    "### \u270f\ufe0f Task 0.3: Pairplot\n",
    "\n",
    "Create a `seaborn.pairplot` of all 4 features, colored by species name (`hue='species_name'`).\n",
    "\n",
    "> This visualization shows every 2D combination of features at once. It is a powerful tool for understanding multivariate structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bb06ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f6a4b5",
   "metadata": {},
   "source": [
    "### Theory: Train/Test Split and Feature Scaling\n",
    "\n",
    "Before training any model, we must:\n",
    "\n",
    "**1. Split the data** into training and test sets:\n",
    "- We train on one portion and evaluate on another \u2192 honest estimate of generalization\n",
    "- Common splits: 70/30 or 80/20\n",
    "- Use **stratified splitting** to maintain class proportions\n",
    "\n",
    "**2. Scale the features** using `StandardScaler`:\n",
    "\n",
    "$$z = \\frac{x - \\mu}{\\sigma}$$\n",
    "\n",
    "This transforms each feature to have **mean = 0** and **std = 1**.\n",
    "\n",
    "> \u26a0\ufe0f **Why scale?** Both KNN and SVM are **distance-based** algorithms. Without scaling, features with larger ranges dominate the distance computation.\n",
    "\n",
    "> \u26a0\ufe0f **Important**: Always `fit` the scaler on the **training data only**, then `transform` both training and test data. This prevents **data leakage**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3da6997",
   "metadata": {},
   "source": [
    "### \u270f\ufe0f Task 0.4: Split and Scale the Data\n",
    "\n",
    "1. Separate features (`X`) and target (`y`) from the DataFrame\n",
    "2. Split into **70% training** and **30% test**, with `random_state=42` and `stratify=y`\n",
    "3. Apply `StandardScaler`: **fit on training data**, transform both training and test\n",
    "4. Print the number of samples in each set\n",
    "5. Print feature means and stds after scaling to verify they are \u2248 0 and \u2248 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9d298e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c386a94",
   "metadata": {},
   "source": [
    "### \u270f\ufe0f Task 0.5: Visualize Scaled vs Unscaled Data\n",
    "\n",
    "Create a figure with **two box plots side by side**:\n",
    "\n",
    "1. **Left**: Box plot of the **unscaled** training features\n",
    "2. **Right**: Box plot of the **scaled** training features\n",
    "\n",
    "> This visualization should make it clear why scaling matters \u2014 the unscaled features have very different ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a4c4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340436ca",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83d\udd35 Part 1: K-Nearest Neighbors (KNN)\n",
    "\n",
    "### Theory\n",
    "\n",
    "**K-Nearest Neighbors** is one of the simplest and most intuitive machine learning algorithms. It is a **lazy learner**: it does not build a model during training \u2014 it stores the entire training set and computes predictions at test time.\n",
    "\n",
    "**Algorithm:**\n",
    "\n",
    "```\n",
    "1. Choose a value for K (number of neighbors)\n",
    "2. For a new point, compute the distance to every training point\n",
    "3. Select the K nearest training points\n",
    "4. Take a majority vote among those K neighbors\n",
    "5. Assign the winning class\n",
    "```\n",
    "\n",
    "**Distance metric** (Euclidean distance):\n",
    "\n",
    "$$d(p, q) = \\sqrt{\\sum_{i=1}^{n}(p_i - q_i)^2}$$\n",
    "\n",
    "**The role of K \u2014 the bias-variance tradeoff:**\n",
    "\n",
    "| K value | Behavior | Risk |\n",
    "|---------|----------|------|\n",
    "| Small K (e.g., 1) | Highly sensitive to individual points | **Overfitting** (high variance) |\n",
    "| Large K (e.g., 100) | Very smooth boundaries | **Underfitting** (high bias) |\n",
    "| $K \\approx \\sqrt{n}$ | Common starting heuristic | Balance \u2014 but always experiment |\n",
    "\n",
    "> \u26a0\ufe0f Because KNN uses distances, **feature scaling is essential**.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/e/e7/KnnClassification.svg/440px-KnnClassification.svg.png\" width=\"300\">\n",
    "\n",
    "*KNN classification: the green dot is classified based on its neighbors (Wikimedia Commons)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bef5ed2",
   "metadata": {},
   "source": [
    "### \u270f\ufe0f Task 1.1: Train a KNN Classifier\n",
    "\n",
    "1. Import `KNeighborsClassifier` from `sklearn.neighbors`\n",
    "2. Create a KNN model with `n_neighbors=5`\n",
    "3. Fit it on the **scaled** training data\n",
    "4. Predict on the **scaled** test data\n",
    "5. Compute and print the accuracy using `accuracy_score` from `sklearn.metrics`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff30d76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0997ad3f",
   "metadata": {},
   "source": [
    "### \u270f\ufe0f Task 1.2: Find the Best K\n",
    "\n",
    "1. Try all values of K from **1 to 30**\n",
    "2. For each K, train a KNN model and compute the test accuracy\n",
    "3. **Plot** accuracy vs K (line plot with markers)\n",
    "4. Add a horizontal dashed red line at the best accuracy value\n",
    "5. Print the best K and its accuracy\n",
    "\n",
    "> **Hint**: Use a `for` loop and store accuracies in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509d0733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0f1b91",
   "metadata": {},
   "source": [
    "### \u270f\ufe0f Task 1.3: Visualize KNN Decision Boundary (2D)\n",
    "\n",
    "Using only the **petal features** (columns 2 and 3 of the scaled data):\n",
    "\n",
    "1. Train KNN (K=5) on the 2D petal features\n",
    "2. Create a **mesh grid** with `np.meshgrid` and step size 0.02\n",
    "3. Predict the class for every point in the mesh\n",
    "4. Plot the decision regions using `contourf` and overlay the training points\n",
    "\n",
    "> This gives a visual intuition of how KNN partitions the feature space.\n",
    "\n",
    "**Hint for mesh grid:**\n",
    "```python\n",
    "h = 0.02\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467b8c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b3d1b2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83d\udfe2 Part 2: Support Vector Machine (SVM)\n",
    "\n",
    "### Theory\n",
    "\n",
    "**Support Vector Machines** find the hyperplane that separates classes with the **maximum margin** \u2014 the largest possible gap between the decision boundary and the closest training points.\n",
    "\n",
    "**Key Concepts:**\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **Support Vectors** | Training points closest to the boundary \u2014 only these define the model |\n",
    "| **Margin** | Distance between the boundary and the nearest support vectors |\n",
    "| **Hyperplane** | The decision surface ($w \\cdot x + b = 0$ in the linear case) |\n",
    "| **Kernel Trick** | Implicitly maps data to higher dimensions for non-linear separation |\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/7/72/SVM_margin.png/300px-SVM_margin.png\" width=\"320\">\n",
    "\n",
    "*SVM maximizes the margin between classes. Support vectors are circled. (Wikimedia Commons)*\n",
    "\n",
    "**Common Kernels:**\n",
    "\n",
    "| Kernel | Formula | Best for |\n",
    "|--------|---------|----------|\n",
    "| **Linear** | $K(x_i, x_j) = x_i \\cdot x_j$ | Linearly separable data |\n",
    "| **RBF** (Gaussian) | $K(x_i, x_j) = \\exp(-\\gamma \\|x_i - x_j\\|^2)$ | Non-linear boundaries (most common) |\n",
    "| **Polynomial** | $K(x_i, x_j) = (\\gamma \\, x_i \\cdot x_j + r)^d$ | Polynomial decision surfaces |\n",
    "\n",
    "**Key Hyperparameters:**\n",
    "\n",
    "| Parameter | Low value | High value |\n",
    "|-----------|-----------|------------|\n",
    "| **C** (regularization) | Smooth boundary \u2192 risk underfitting | Complex boundary \u2192 risk overfitting |\n",
    "| **gamma** (RBF/poly) | Long-range influence \u2192 risk underfitting | Short-range influence \u2192 risk overfitting |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11173b00",
   "metadata": {},
   "source": [
    "### \u270f\ufe0f Task 2.1: Train SVMs with Different Kernels\n",
    "\n",
    "1. Import `SVC` from `sklearn.svm`\n",
    "2. Train three SVM models with kernels: `'linear'`, `'rbf'`, `'poly'` (use `random_state=42`)\n",
    "3. Predict on the test set with each model\n",
    "4. Print the accuracy of each model\n",
    "\n",
    "> **Question**: Which kernel performs best? Is the difference large?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a7d736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f068776",
   "metadata": {},
   "source": [
    "### \u270f\ufe0f Task 2.2: Visualize Support Vectors\n",
    "\n",
    "Using only the **petal features** (columns 2 and 3 of the scaled data) for visualization:\n",
    "\n",
    "1. Train an SVM with `kernel='rbf'` on the 2D petal features\n",
    "2. Create a scatter plot of the training data colored by class\n",
    "3. **Highlight the support vectors** with larger, hollow red circles (`facecolors='none'`, `edgecolors='red'`)\n",
    "4. Print the total number of support vectors and the count per class\n",
    "\n",
    "> **Hint**: After fitting, access `model.support_vectors_` and `model.n_support_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4d37c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fea095",
   "metadata": {},
   "source": [
    "### \u270f\ufe0f Task 2.3: Effect of the C Parameter\n",
    "\n",
    "Explore how the regularization parameter C affects the decision boundary:\n",
    "\n",
    "1. Use C values: `[0.01, 0.1, 1, 10, 100]`\n",
    "2. For each C, train an SVM (`kernel='rbf'`) on the 2D petal features\n",
    "3. Plot the **decision boundary** using `contourf` on a mesh grid\n",
    "4. Overlay the training points\n",
    "5. In each subplot title, show: C value, test accuracy, number of support vectors\n",
    "\n",
    "> Create a figure with **5 subplots in a row** (`1 x 5`), one per C value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5312a0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ebfb16",
   "metadata": {},
   "source": [
    "### \u270f\ufe0f Task 2.4: Visualize SVM Decision Boundary (2D)\n",
    "\n",
    "Using the **petal features** and the best-performing kernel:\n",
    "\n",
    "1. Train SVM on 2D petal features\n",
    "2. Plot the decision boundary using `contourf` (same mesh technique as Task 1.3)\n",
    "3. Overlay training points and highlight support vectors\n",
    "\n",
    "> Compare this boundary visually with the KNN boundary from Task 1.3. Which looks smoother?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da36160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dff56eb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83d\udfe1 Part 3: K-Means Clustering\n",
    "\n",
    "### Theory\n",
    "\n",
    "Now we switch from **supervised** to **unsupervised** learning. K-Means partitions data into K clusters **without using labels**.\n",
    "\n",
    "**Algorithm:**\n",
    "\n",
    "```\n",
    "1. INITIALIZE: Place K centroids randomly\n",
    "2. ASSIGN:     Assign each point to the nearest centroid\n",
    "3. UPDATE:     Move each centroid to the mean of its assigned points\n",
    "4. REPEAT:     Steps 2-3 until convergence (centroids stop moving)\n",
    "```\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/K-means_convergence.gif/440px-K-means_convergence.gif\" width=\"350\">\n",
    "\n",
    "*K-Means convergence animation: centroids move iteratively until stable (Wikimedia Commons)*\n",
    "\n",
    "**Properties:**\n",
    "- You must specify **K** in advance\n",
    "- Sensitive to initialization \u2192 use `n_init=10` (run multiple times, keep best)\n",
    "- Assumes **spherical, equally-sized** clusters\n",
    "- **Feature scaling** is important (distance-based)\n",
    "\n",
    "**How to choose K:**\n",
    "\n",
    "| Method | How it works | What to look for |\n",
    "|--------|-------------|-----------------|\n",
    "| **Elbow Method** | Plot inertia (within-cluster sum of squares) vs K | The \"elbow\" where inertia stops dropping sharply |\n",
    "| **Silhouette Score** | Measures cluster cohesion vs separation `[-1, 1]` | The K with the highest score |\n",
    "\n",
    "> \u26a0\ufe0f Always combine algorithmic criteria with **domain knowledge**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78f490f",
   "metadata": {},
   "source": [
    "### \u270f\ufe0f Task 3.1: Elbow Method\n",
    "\n",
    "1. Import `KMeans` from `sklearn.cluster`\n",
    "2. Try K values from **1 to 10**\n",
    "3. For each K, train K-Means (with `random_state=42`, `n_init=10`) and record `inertia_`\n",
    "4. **Plot** inertia vs K with markers\n",
    "5. Annotate the elbow point on the plot\n",
    "\n",
    "> **Question**: Where is the elbow? Does it match the true number of species?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089a19d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d5f818",
   "metadata": {},
   "source": [
    "### \u270f\ufe0f Task 3.2: Silhouette Score Analysis\n",
    "\n",
    "1. Import `silhouette_score` from `sklearn.metrics`\n",
    "2. For K values from **2 to 10**, compute the silhouette score\n",
    "3. **Plot** silhouette score vs K\n",
    "4. Print all scores and identify the best K\n",
    "\n",
    "> **Question**: Does the silhouette method agree with the elbow method? If not, why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2996900a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2da06d3",
   "metadata": {},
   "source": [
    "### \u270f\ufe0f Task 3.3: Compare Clusters with True Labels\n",
    "\n",
    "1. Train K-Means with **K=3** on the scaled training data\n",
    "2. Create a figure with **two scatter plots side by side** (using petal features):\n",
    "   - **Left**: True species labels\n",
    "   - **Right**: K-Means cluster assignments, with centroids plotted as large red **X** markers\n",
    "3. Create a **cross-tabulation** (`pd.crosstab`) comparing true labels with cluster assignments\n",
    "4. Print and analyze the table\n",
    "\n",
    "> **Question**: How well did K-Means recover the true species grouping?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542dbbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcad7c14",
   "metadata": {},
   "source": [
    "### \u270f\ufe0f Task 3.4: Visualize Cluster Silhouette per Sample\n",
    "\n",
    "Using `sklearn.metrics.silhouette_samples`, create a **silhouette plot** for K=3:\n",
    "\n",
    "1. Compute per-sample silhouette values\n",
    "2. For each cluster, plot horizontal bars sorted by silhouette value\n",
    "3. Add a vertical dashed line at the mean silhouette score\n",
    "4. Color each cluster differently\n",
    "\n",
    "> This plot reveals whether all clusters are well-formed or if some contain poorly-assigned points.\n",
    "\n",
    "**Hint:**\n",
    "```python\n",
    "from sklearn.metrics import silhouette_samples\n",
    "sample_silhouette_values = silhouette_samples(X_train_scaled, cluster_labels)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da60e613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f601a013",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83d\udcca Part 4: Evaluation Metrics\n",
    "\n",
    "### Theory: Classification Metrics (Supervised)\n",
    "\n",
    "| Metric | Formula | What it answers |\n",
    "|--------|---------|----------------|\n",
    "| **Precision** | $\\frac{TP}{TP + FP}$ | Of predicted positives, how many are actually positive? |\n",
    "| **Recall** | $\\frac{TP}{TP + FN}$ | Of actual positives, how many did we find? |\n",
    "| **F1-Score** | $2 \\cdot \\frac{P \\cdot R}{P + R}$ | Harmonic mean \u2014 balances precision and recall |\n",
    "| **Accuracy** | $\\frac{TP + TN}{Total}$ | Overall fraction of correct predictions |\n",
    "\n",
    "**Averaging strategies for multi-class:**\n",
    "- **Macro**: Average across classes (treats all classes equally)\n",
    "- **Weighted**: Weighted by support (accounts for imbalance)\n",
    "\n",
    "**Confusion Matrix**: An $n \\times n$ table where rows = true classes, columns = predictions. Diagonal = correct, off-diagonal = errors.\n",
    "\n",
    "### Theory: Clustering Metrics (Unsupervised)\n",
    "\n",
    "| Metric | Range | What it measures |\n",
    "|--------|-------|-----------------|\n",
    "| **Silhouette Score** | $[-1, 1]$ | How similar a point is to its own cluster vs other clusters |\n",
    "| **Adjusted Rand Index** | $[-1, 1]$ | Agreement between cluster labels and true labels (1 = perfect) |\n",
    "| **Inertia** | $[0, \\infty)$ | Within-cluster sum of squares (lower = tighter clusters) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d675195",
   "metadata": {},
   "source": [
    "### \u270f\ufe0f Task 4.1: Classification Reports\n",
    "\n",
    "1. Import `classification_report` from `sklearn.metrics`\n",
    "2. Print the **classification report** for KNN (K=5) predictions\n",
    "3. Print the **classification report** for the best SVM model\n",
    "4. Use `target_names=iris.target_names` for readable output\n",
    "\n",
    "> **Question**: Which class has the highest F1-score? Which has the lowest? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e364b2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec1dba6",
   "metadata": {},
   "source": [
    "### \u270f\ufe0f Task 4.2: Confusion Matrices\n",
    "\n",
    "1. Import `ConfusionMatrixDisplay` from `sklearn.metrics`\n",
    "2. Create a figure with **2 confusion matrices side by side**:\n",
    "   - Left: KNN predictions (`cmap='Blues'`)\n",
    "   - Right: Best SVM predictions (`cmap='Greens'`)\n",
    "3. Use `display_labels=iris.target_names`\n",
    "\n",
    "> **Question**: Which species pair causes the most confusion? Is this consistent with the scatter plots?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e27b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9362c5f3",
   "metadata": {},
   "source": [
    "### \u270f\ufe0f Task 4.3: Clustering Evaluation\n",
    "\n",
    "Evaluate K-Means clustering (K=3):\n",
    "\n",
    "1. Compute **Silhouette Score** on the training data\n",
    "2. Compute **Adjusted Rand Index** comparing `y_train` with cluster labels\n",
    "3. Report **Inertia** from `kmeans.inertia_`\n",
    "4. Print all three metrics with a brief interpretation of each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89593b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e9e938",
   "metadata": {},
   "source": [
    "### \u270f\ufe0f Task 4.4: Precision-Recall Bar Chart\n",
    "\n",
    "Create a **grouped bar chart** comparing precision and recall for each class, for both KNN and SVM:\n",
    "\n",
    "1. Extract per-class precision and recall from the classification reports (use `classification_report(..., output_dict=True)`)\n",
    "2. Plot grouped bars: 3 classes \u00d7 2 metrics \u00d7 2 models\n",
    "3. Add a legend and axis labels\n",
    "\n",
    "> This visualization makes it easier to spot which classes are harder for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053157de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7460ad",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83c\udfc6 Part 5: Model Comparison\n",
    "\n",
    "### \u270f\ufe0f Task 5.1: Build a Comparison Table\n",
    "\n",
    "Create a `pandas.DataFrame` comparing all supervised models with these columns:\n",
    "- Model name\n",
    "- Accuracy\n",
    "- Macro F1-Score\n",
    "\n",
    "Sort by accuracy (descending) and display the table.\n",
    "\n",
    "**Models to include**: KNN (K=5), SVM (Linear), SVM (RBF), SVM (Poly)\n",
    "\n",
    "**Hint**: Use `f1_score(y_test, y_pred, average='macro')` from `sklearn.metrics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7bdb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b925871",
   "metadata": {},
   "source": [
    "### \u270f\ufe0f Task 5.2: Comparison Bar Chart\n",
    "\n",
    "Create a **grouped bar chart** comparing accuracy and macro F1-score across all 4 models.\n",
    "\n",
    "> Visualizing the comparison table makes differences immediately obvious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090f8099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8f8220",
   "metadata": {},
   "source": [
    "### \u270f\ufe0f Task 5.3: Discussion\n",
    "\n",
    "Answer the following questions (in a markdown cell or as print statements):\n",
    "\n",
    "1. Which algorithm performed best on this dataset? Was the difference significant?\n",
    "2. In what scenarios would you prefer **KNN** over **SVM**, and vice versa?\n",
    "3. How well did **K-Means** recover the true clusters without labels?\n",
    "4. Why is **accuracy alone** not sufficient to evaluate a classifier?\n",
    "5. What would change if the dataset were **highly imbalanced** (e.g., 90% setosa)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb96fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  }
 ]
}